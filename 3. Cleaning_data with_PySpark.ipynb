{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrame details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Data Cleaning?\n",
    "Preparing raw data for use in data processing pipelines.\n",
    "\n",
    "Possible tasks in data cleaning: \n",
    "* Reformatting or replacing text \n",
    "* Performing calculations\n",
    "* Removing garbage or incomplete data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a schema\n",
    "Creating a defined schema helps with data quality and import performance. As mentioned during the lesson, we'll create a simple schema to read in the following columns:\n",
    "* Name\n",
    "* Age\n",
    "* City\n",
    "\n",
    "The `Name` and `City` columns are StringType() and the `Age` column is an `IntegerType()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Import `*` from the `pyspark.sql.types` library.\n",
    "* Define a new schema using the `StructType` method.\n",
    "* Define a `StructField` for `name`, `age`, and `city`. Each field should correspond to the correct datatype and not be nullable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T11:20:43.677826Z",
     "start_time": "2021-02-18T11:20:43.672308Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import the pyspark.sql.types library\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Define a new schema using the StructType method\n",
    "people_schema = StructType([\n",
    "  # Define a StructField for each field\n",
    "  StructField('name', StringType(), False),\n",
    "  StructField('age', IntegerType(), False),\n",
    "  StructField('city', StringType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Immutability\n",
    "Unlike typical Python variables, Spark Data Frames are immutable. While not strictly required, immutability is often a component of functional programming. We won't go into everything that implies here, but understand that Spark is designed to use immutable objects. Practically, this means Spark Data Frames are defined once and are not modifiable after initialization. If the variable name is reused, the original data is removed (assuming it's not in use elsewhere) and the variable name is reassigned to the new data. While this seems inefficient, it actually allows Spark to share data between all cluster components. It can do so without worry about concurrent data objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lazy Processing\n",
    "You may be wondering how Spark does this so quickly, especially on large data sets. Spark can do this because of something called lazy processing. Lazy processing in Spark is the idea that very little actually happens until an action is performed. In our previous example, we read a CSV file, added a new column, and deleted another. The trick is that no data was actually read / added / modified, we only updated the instructions (aka, Transformations) for what we wanted Spark to do. This functionality allows Spark to perform the most efficient set of operations to get the desired result. The code example is the same as the previous slide, but with the added count() method call. This classifies as an action in Spark and will process all the transformation operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using lazy processing\n",
    "Lazy processing operations will usually return in about the same amount of time regardless of the actual quantity of data. Remember that this is due to Spark not performing any transformations until an action is requested.\n",
    "\n",
    "For this exercise, we'll be defining a Data Frame (`aa_dfw_df`) and add a couple transformations. Note the amount of time required for the transformations to complete when defined vs when the data is actually queried. These differences may be short, but they will be noticeable. When working with a full Spark cluster with larger quantities of data the difference will be more apparent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Load the Data Frame.\n",
    "* Add the transformation for `F.lower()` to the `Destination Airport` column.\n",
    "* Drop the `Destination Airport` column from the Data Frame `aa_dfw_df`. Note the time for these operations to complete.\n",
    "* Show the Data Frame, noting the time difference for this action to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T11:53:24.394262Z",
     "start_time": "2021-02-18T11:53:04.792347Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T11:59:48.404454Z",
     "start_time": "2021-02-18T11:59:48.394954Z"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T11:59:55.394089Z",
     "start_time": "2021-02-18T11:59:50.668078Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "aa_dfw_df = pd.read_csv(\"AA_DFW_2017_Departures_Short.csv\")\n",
    "aa_dfw_df = spark.createDataFrame(aa_dfw_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file\n",
    "# aa_dfw_df = spark.read.format('csv').options(Header=True).load('AA_DFW_2017_Departures_Short.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T12:00:07.676943Z",
     "start_time": "2021-02-18T11:59:57.412397Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------+-------------------+-----------------------------+\n",
      "|Date (MM/DD/YYYY)|Flight Number|Destination Airport|Actual elapsed time (Minutes)|\n",
      "+-----------------+-------------+-------------------+-----------------------------+\n",
      "|       01/01/2017|            5|                HNL|                          537|\n",
      "|       01/01/2017|            7|                OGG|                          498|\n",
      "|       01/01/2017|           37|                SFO|                          241|\n",
      "|       01/01/2017|           43|                DTW|                          134|\n",
      "|       01/01/2017|           51|                STL|                           88|\n",
      "|       01/01/2017|           60|                MIA|                          149|\n",
      "|       01/01/2017|           71|                LAX|                          203|\n",
      "|       01/01/2017|           74|                MEM|                           76|\n",
      "|       01/01/2017|           81|                DEN|                          123|\n",
      "|       01/01/2017|           89|                SLC|                          161|\n",
      "|       01/01/2017|           96|                STL|                           84|\n",
      "|       01/01/2017|          103|                SJC|                          216|\n",
      "|       01/01/2017|          119|                OGG|                          514|\n",
      "|       01/01/2017|          123|                HNL|                          529|\n",
      "|       01/01/2017|          126|                LGA|                          171|\n",
      "|       01/01/2017|          132|                EWR|                          188|\n",
      "|       01/01/2017|          140|                SJC|                          231|\n",
      "|       01/01/2017|          174|                RDU|                          145|\n",
      "|       01/01/2017|          176|                BOS|                          184|\n",
      "|       01/01/2017|          190|                SAT|                           76|\n",
      "+-----------------+-------------+-------------------+-----------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "aa_dfw_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T12:00:11.167026Z",
     "start_time": "2021-02-18T12:00:10.449842Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------+-----------------------------+-------+\n",
      "|Date (MM/DD/YYYY)|Flight Number|Actual elapsed time (Minutes)|airport|\n",
      "+-----------------+-------------+-----------------------------+-------+\n",
      "|       01/01/2017|            5|                          537|    hnl|\n",
      "|       01/01/2017|            7|                          498|    ogg|\n",
      "|       01/01/2017|           37|                          241|    sfo|\n",
      "|       01/01/2017|           43|                          134|    dtw|\n",
      "|       01/01/2017|           51|                           88|    stl|\n",
      "|       01/01/2017|           60|                          149|    mia|\n",
      "|       01/01/2017|           71|                          203|    lax|\n",
      "|       01/01/2017|           74|                           76|    mem|\n",
      "|       01/01/2017|           81|                          123|    den|\n",
      "|       01/01/2017|           89|                          161|    slc|\n",
      "|       01/01/2017|           96|                           84|    stl|\n",
      "|       01/01/2017|          103|                          216|    sjc|\n",
      "|       01/01/2017|          119|                          514|    ogg|\n",
      "|       01/01/2017|          123|                          529|    hnl|\n",
      "|       01/01/2017|          126|                          171|    lga|\n",
      "|       01/01/2017|          132|                          188|    ewr|\n",
      "|       01/01/2017|          140|                          231|    sjc|\n",
      "|       01/01/2017|          174|                          145|    rdu|\n",
      "|       01/01/2017|          176|                          184|    bos|\n",
      "|       01/01/2017|          190|                           76|    sat|\n",
      "+-----------------+-------------+-----------------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add the airport column using the F.lower() method\n",
    "aa_dfw_df = aa_dfw_df.withColumn('airport', F.lower(aa_dfw_df['Destination Airport']))\n",
    "\n",
    "# Drop the Destination Airport column\n",
    "aa_dfw_df = aa_dfw_df.drop(aa_dfw_df['Destination Airport'])\n",
    "\n",
    "# Show the DataFrame\n",
    "aa_dfw_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding Parquet\n",
    "Welcome back! As we've seen, Spark can read in text and CSV files. While this gives us access to many data sources, it's not always the most convenient format to work with. Let's take a look at a few problems with CSV files.\n",
    "\n",
    "## 2. Difficulties with CSV files\n",
    "Some common issues with CSV files include: The schema is not defined: there are no data types included, nor column names (beyond a header row). Using content containing a comma (or another delimiter) requires escaping. Using the escape character within content requires even further escaping. The available encoding formats are limited depending on the language used.\n",
    "\n",
    "## 3. Spark and CSV files\n",
    "In addition to the issues with CSV files in general, Spark has some specific problems processing CSV data. CSV files are quite slow to import and parse. The files cannot be shared between workers during the import process. If no schema is defined, all data must be read before a schema can be inferred. Spark has feature known as predicate pushdown. Basically, this is the idea of ordering tasks to do the least amount of work. Filtering data prior to processing is one of the primary optimizations of predicate pushdown. This drastically reduces the amount of information that must be processed in large data sets. Unfortunately, you cannot filter the CSV data via predicate pushdown. Finally, Spark processes are often multi-step and may utilize an intermediate file representation. These representations allow data to be used later without regenerating the data from source. Using CSV would instead require a significant amount of extra work defining schemas, encoding formats, etc.\n",
    "\n",
    "## 4. The Parquet Format\n",
    "Parquet is a compressed columnar data format developed for use in any Hadoop based system. This includes Spark, Hadoop, Apache Impala, and so forth. The Parquet format is structured with data accessible in chunks, allowing efficient read / write operations without processing the entire file. This structured format supports Spark's predicate pushdown functionality, providing significant performance improvement. Finally, Parquet files automatically include schema information and handle data encoding. This is perfect for intermediary or on-disk representation of processed data. Note that Parquet files are a binary file format and can only be used with the proper tools. This is in contrast to CSV files which can be edited with any text editor.\n",
    "\n",
    "## 5. Working with Parquet\n",
    "Interacting with Parquet files is very straightforward. To read a parquet file into a Data Frame, you have two options. The first is using the `spark.read.format` method we've seen previously. \n",
    "\n",
    "1) `df=spark.read.format('parquet').load('filename.parquet')`<br>\n",
    "2) `df=spark.read.parquet('filename.parquet')` \n",
    "\n",
    "Typically, the shortcut version is the easiest to use but you can use them interchangeably. Writing parquet files is similar, using either:\n",
    "\n",
    "1) `df.write.format('parquet').save('filename.parquet')` or <br>\n",
    "2) `df.write.parquet('filename.parquet')` \n",
    "\n",
    "The long-form versions of each permit extra option flags, such as when overwriting an existing parquet file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Parquet and SQL\n",
    "Parquet files have various uses within Spark. We've discussed using them as an intermediate data format, but they also are perfect for performing SQL operations. To perform a SQL query against a Parquet file, we first need to create a Data Frame via the spark.read.parquet method. Once we have the Data Frame, we can use the `createOrReplaceTempView()` method to add an alias of the Parquet data as a SQL table. Finally, we run our query using normal SQL syntax and the spark.sql method. In this case, we're looking for all flights with a duration under 100 minutes. Because we're using Parquet as the backing store, we get all the performance benefits we've discussed previously (primarily defined schemas and the available use of predicate pushdown)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving a DataFrame in Parquet format\n",
    "When working with Spark, you'll often start with CSV, JSON, or other data sources. This provides a lot of flexibility for the types of data to load, but it is not an optimal format for Spark. The `Parquet` format is a columnar data store, allowing Spark to use predicate pushdown. This means Spark will only process the data necessary to complete the operations you define versus reading the entire dataset. This gives Spark more flexibility in accessing the data and often drastically improves performance on large datasets.\n",
    "\n",
    "In this exercise, we're going to practice creating a new Parquet file and then process some data from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T12:00:25.372475Z",
     "start_time": "2021-02-18T12:00:21.411316Z"
    }
   },
   "outputs": [],
   "source": [
    "# df1 = spark.read.format('csv').options(Header=True).load('AA_DFW_2017_Departures_Short.csv.gz')\n",
    "df1 = pd.read_csv(\"AA_DFW_2017_Departures_Short.csv\")\n",
    "df1 = spark.createDataFrame(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T12:06:09.870909Z",
     "start_time": "2021-02-18T12:06:05.883060Z"
    }
   },
   "outputs": [],
   "source": [
    "# df2 = spark.read.format('csv').options(Header=True).load('AA_DFW_2016_Departures_Short.csv.gz')\n",
    "df2 = pd.read_csv(\"AA_DFW_2016_Departures_Short.csv\")\n",
    "df2 = spark.createDataFrame(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T12:18:41.072018Z",
     "start_time": "2021-02-18T12:18:31.251591Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df1 Count: 139358\n",
      "df2 Count: 140604\n"
     ]
    }
   ],
   "source": [
    "# View the row count of df1 and df2\n",
    "print(\"df1 Count: %d\" % df1.count())\n",
    "print(\"df2 Count: %d\" % df2.count())\n",
    "\n",
    "# Combine the DataFrames into one\n",
    "df3 = df1.union(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T12:05:49.847257Z",
     "start_time": "2021-02-18T12:05:41.723121Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------+-------------------+-----------------------------+\n",
      "|Date (MM/DD/YYYY)|Flight Number|Destination Airport|Actual elapsed time (Minutes)|\n",
      "+-----------------+-------------+-------------------+-----------------------------+\n",
      "|       01/01/2017|            5|                HNL|                          537|\n",
      "|       01/01/2017|            7|                OGG|                          498|\n",
      "|       01/01/2017|           37|                SFO|                          241|\n",
      "|       01/01/2017|           43|                DTW|                          134|\n",
      "|       01/01/2017|           51|                STL|                           88|\n",
      "|       01/01/2017|           60|                MIA|                          149|\n",
      "|       01/01/2017|           71|                LAX|                          203|\n",
      "|       01/01/2017|           74|                MEM|                           76|\n",
      "|       01/01/2017|           81|                DEN|                          123|\n",
      "|       01/01/2017|           89|                SLC|                          161|\n",
      "|       01/01/2017|           96|                STL|                           84|\n",
      "|       01/01/2017|          103|                SJC|                          216|\n",
      "|       01/01/2017|          119|                OGG|                          514|\n",
      "|       01/01/2017|          123|                HNL|                          529|\n",
      "|       01/01/2017|          126|                LGA|                          171|\n",
      "|       01/01/2017|          132|                EWR|                          188|\n",
      "|       01/01/2017|          140|                SJC|                          231|\n",
      "|       01/01/2017|          174|                RDU|                          145|\n",
      "|       01/01/2017|          176|                BOS|                          184|\n",
      "|       01/01/2017|          190|                SAT|                           76|\n",
      "+-----------------+-------------+-------------------+-----------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T12:06:15.575852Z",
     "start_time": "2021-02-18T12:06:15.136198Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------+-------------------+-----------------------------+\n",
      "|Date (MM/DD/YYYY)|Flight Number|Destination Airport|Actual elapsed time (Minutes)|\n",
      "+-----------------+-------------+-------------------+-----------------------------+\n",
      "|       01/01/2016|            5|                HNL|                          529|\n",
      "|       01/01/2016|            7|                OGG|                          512|\n",
      "|       01/01/2016|           25|                PHL|                          161|\n",
      "|       01/01/2016|           37|                SFO|                          259|\n",
      "|       01/01/2016|           43|                DTW|                          157|\n",
      "|       01/01/2016|           60|                MIA|                          144|\n",
      "|       01/01/2016|           71|                LAS|                          165|\n",
      "|       01/01/2016|           79|                SLC|                          153|\n",
      "|       01/01/2016|           81|                TUS|                          133|\n",
      "|       01/01/2016|           98|                SMF|                          207|\n",
      "|       01/01/2016|          103|                SJC|                          231|\n",
      "|       01/01/2016|          119|                OGG|                          508|\n",
      "|       01/01/2016|          122|                MCO|                          137|\n",
      "|       01/01/2016|          123|                HNL|                          532|\n",
      "|       01/01/2016|          128|                IAD|                          144|\n",
      "|       01/01/2016|          132|                PDX|                          223|\n",
      "|       01/01/2016|          138|                DCA|                          148|\n",
      "|       01/01/2016|          140|                SJC|                          223|\n",
      "|       01/01/2016|          141|                LAX|                          191|\n",
      "|       01/01/2016|          143|                BOS|                          186|\n",
      "+-----------------+-------------+-------------------+-----------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T12:09:25.225600Z",
     "start_time": "2021-02-18T12:09:24.612453Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------+-------------------+-----------------------------+\n",
      "|Date (MM/DD/YYYY)|Flight Number|Destination Airport|Actual elapsed time (Minutes)|\n",
      "+-----------------+-------------+-------------------+-----------------------------+\n",
      "|       01/01/2017|            5|                HNL|                          537|\n",
      "|       01/01/2017|            7|                OGG|                          498|\n",
      "|       01/01/2017|           37|                SFO|                          241|\n",
      "|       01/01/2017|           43|                DTW|                          134|\n",
      "|       01/01/2017|           51|                STL|                           88|\n",
      "|       01/01/2017|           60|                MIA|                          149|\n",
      "|       01/01/2017|           71|                LAX|                          203|\n",
      "|       01/01/2017|           74|                MEM|                           76|\n",
      "|       01/01/2017|           81|                DEN|                          123|\n",
      "|       01/01/2017|           89|                SLC|                          161|\n",
      "|       01/01/2017|           96|                STL|                           84|\n",
      "|       01/01/2017|          103|                SJC|                          216|\n",
      "|       01/01/2017|          119|                OGG|                          514|\n",
      "|       01/01/2017|          123|                HNL|                          529|\n",
      "|       01/01/2017|          126|                LGA|                          171|\n",
      "|       01/01/2017|          132|                EWR|                          188|\n",
      "|       01/01/2017|          140|                SJC|                          231|\n",
      "|       01/01/2017|          174|                RDU|                          145|\n",
      "|       01/01/2017|          176|                BOS|                          184|\n",
      "|       01/01/2017|          190|                SAT|                           76|\n",
      "+-----------------+-------------+-------------------+-----------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T12:18:41.084934Z",
     "start_time": "2021-02-18T12:18:41.073835Z"
    }
   },
   "outputs": [],
   "source": [
    "df3 = df3.withColumnRenamed(\"Date (MM/DD/YYYY)\", \"date\")\n",
    "df3 = df3.withColumnRenamed(\"Flight Number\", \"flight_number\")\n",
    "df3 = df3.withColumnRenamed(\"Destination Airport\", \"destination_airport\")\n",
    "df3 = df3.withColumnRenamed(\"Actual elapsed time (Minutes)\", \"flight_duration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T12:18:46.215350Z",
     "start_time": "2021-02-18T12:18:41.086967Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "279962\n"
     ]
    }
   ],
   "source": [
    "# Save the df3 DataFrame in Parquet format\n",
    "df3.write.parquet('AA_DFW_ALL.parquet', mode='overwrite')\n",
    "\n",
    "# Read the Parquet file into a new DataFrame and run a count\n",
    "print(spark.read.parquet('AA_DFW_ALL.parquet').count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL and Parquet\n",
    "Parquet files are perfect as a backing data store for SQL queries in Spark. While it is possible to run the same queries directly via Spark's Python functions, sometimes it's easier to run SQL queries alongside the Python options.\n",
    "\n",
    "For this example, we're going to read in the Parquet file we created in the last exercise and register it as a SQL table. Once registered, we'll run a quick query against the table (aka, the Parquet file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T12:18:52.894271Z",
     "start_time": "2021-02-18T12:18:52.794511Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read the Parquet file into flights_df\n",
    "flights_df = spark.read.parquet(\"AA_DFW_ALL.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T12:18:54.045517Z",
     "start_time": "2021-02-18T12:18:53.500726Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-------------------+---------------+\n",
      "|      date|flight_number|destination_airport|flight_duration|\n",
      "+----------+-------------+-------------------+---------------+\n",
      "|09/30/2017|         2214|                ABQ|            109|\n",
      "|09/30/2017|         2215|                DEN|            113|\n",
      "|09/30/2017|         2222|                AUS|             49|\n",
      "|09/30/2017|         2226|                DSM|            104|\n",
      "|09/30/2017|         2228|                SMF|            241|\n",
      "|09/30/2017|         2236|                TUL|             59|\n",
      "|09/30/2017|         2248|                BWI|            163|\n",
      "|09/30/2017|         2250|                LAX|            196|\n",
      "|09/30/2017|         2252|                SFO|            223|\n",
      "|09/30/2017|         2258|                IAH|             52|\n",
      "|09/30/2017|         2259|                BNA|             96|\n",
      "|09/30/2017|         2260|                PSP|            180|\n",
      "|09/30/2017|         2262|                LAS|            167|\n",
      "|09/30/2017|         2263|                LGA|            190|\n",
      "|09/30/2017|         2267|                MEM|             84|\n",
      "|09/30/2017|         2268|                CVG|            117|\n",
      "|09/30/2017|         2271|                PBI|            160|\n",
      "|09/30/2017|         2276|                IND|            113|\n",
      "|09/30/2017|         2278|                OKC|             55|\n",
      "|09/30/2017|         2281|                DSM|             99|\n",
      "+----------+-------------+-------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T12:19:24.687836Z",
     "start_time": "2021-02-18T12:19:22.692528Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average flight time is: 151\n"
     ]
    }
   ],
   "source": [
    "# Register the temp table\n",
    "flights_df.createOrReplaceTempView('flights')\n",
    "\n",
    "# Run a SQL query of the average flight duration\n",
    "avg_duration = spark.sql('SELECT avg(flight_duration) from flights').collect()[0]\n",
    "print('The average flight time is: %d' % avg_duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manipulating DataFrames in the real world\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return rows where name starts with \"M\" \n",
    "voter_df.filter(voter_df.name.like('M%'))\n",
    "\n",
    "# Return name and position \n",
    "onlyvoters = voter_df.select('name', 'position')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common DataFrame transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Filter/where"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voter_df.filter(voter_df.date > '1/1/2019') # or voter_df.where(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voter_df.select(voter_df.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* withColumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voter_df.withColumn('year', voter_df.date.year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voter_df.drop('unused_column')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voter_df.filter(voter_df['name'].isNotNull())\n",
    "voter_df.filter(voter_df.date.year > 1800)\n",
    "voter_df.where(voter_df['_c0'].contains('VOTE'))\n",
    "voter_df.where(~ voter_df._c1.isNull())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column string transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Contained in pyspark.sql.functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Applied per column as transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voter_df.withColumn('upper', F.upper('name'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Can create intermediary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voter_df.withColumn('splits', F.split('name', ' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Can cast to other types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voter_df.withColumn('year', voter_df['_c4'].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ArrayType() column functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.size(<column>)` - returns length of arrayType() column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.getItem(<index>)` - used to retrieve a specific item at index of list column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering column content with Python\n",
    "You've looked at using various operations on DataFrame columns - now you can modify a real dataset. The DataFrame `voter_df` contains information regarding the voters on the Dallas City Council from the past few years. This truncated DataFrame contains the date of the vote being cast and the name and position of the voter. Your manager has asked you to clean this data so it can later be integrated into some desired reports. The primary task is to remove any null entries or odd characters and return a specific set of voters where you can validate their information.\n",
    "\n",
    "This is often one of the first steps in data cleaning - removing anything that is obviously outside the format. For this dataset, make sure to look at the original data and see what looks out of place for the `VOTER_NAME` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T12:34:15.355429Z",
     "start_time": "2021-02-18T12:34:15.333514Z"
    }
   },
   "outputs": [],
   "source": [
    "# voter_df = spark.read.format('csv').options(Header=True).load('DallasCouncilVoters.csv.gz')\n",
    "voter_df = pd.read_csv(\"DallasCouncilVoters.csv\")\n",
    "voter_df = spark.createDataFrame(voter_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T12:34:33.507589Z",
     "start_time": "2021-02-18T12:34:25.723428Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-------------------+\n",
      "|      DATE|        TITLE|         VOTER_NAME|\n",
      "+----------+-------------+-------------------+\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|\n",
      "|02/08/2017|Councilmember|       Adam Medrano|\n",
      "|02/08/2017|Councilmember|       Casey Thomas|\n",
      "|02/08/2017|Councilmember|Carolyn King Arnold|\n",
      "|02/08/2017|Councilmember|       Scott Griggs|\n",
      "|02/08/2017|Councilmember|   B. Adam  McGough|\n",
      "|02/08/2017|Councilmember|       Lee Kleinman|\n",
      "|02/08/2017|Councilmember|      Sandy Greyson|\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|\n",
      "|02/08/2017|Councilmember|       Adam Medrano|\n",
      "|02/08/2017|Councilmember|       Casey Thomas|\n",
      "|02/08/2017|Councilmember|Carolyn King Arnold|\n",
      "|02/08/2017|Councilmember| Rickey D. Callahan|\n",
      "|01/11/2017|Councilmember|  Jennifer S. Gates|\n",
      "|04/25/2018|Councilmember|     Sandy  Greyson|\n",
      "|04/25/2018|Councilmember| Jennifer S.  Gates|\n",
      "+----------+-------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "voter_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T12:35:04.211822Z",
     "start_time": "2021-02-18T12:34:59.171947Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------------------+\n",
      "|VOTER_NAME                                                                                                                    |\n",
      "+------------------------------------------------------------------------------------------------------------------------------+\n",
      "|  the  final   2018 Assessment Plan and the 2018 Assessment  Roll  (to  be  kept  on  file  with  the  City  Secretary)       |\n",
      "|Mark  Clayton                                                                                                                 |\n",
      "|Omar Narvaez                                                                                                                  |\n",
      "|Dwaine R. Caraway                                                                                                             |\n",
      "|011018__42                                                                                                                    |\n",
      "|   the   final  2018 Assessment  Plan  and  the  2018 Assessment  Roll  (to  be  kept  on  file   with the City Secretary)    |\n",
      "| the final 2018 Assessment Plan and the 2018 Assessment  Roll  (to  be  kept  on  file  with  the  City  Secretary)           |\n",
      "|Casey Thomas                                                                                                                  |\n",
      "|Sandy  Greyson                                                                                                                |\n",
      "|   the   final  2018 Assessment  Plan  and  the  2018 Assessment  Roll   (to  be  kept  on  file   with the City Secretary)   |\n",
      "|  the  final  2018 Assessment  Plan   and   the   2018 Assessment   Roll  (to  be  kept  on  file  with  the  City  Secretary)|\n",
      "|Tiffinni A. Young                                                                                                             |\n",
      "|Carolyn King Arnold                                                                                                           |\n",
      "|Scott Griggs                                                                                                                  |\n",
      "|Scott  Griggs                                                                                                                 |\n",
      "|Casey  Thomas                                                                                                                 |\n",
      "|B. Adam  McGough                                                                                                              |\n",
      "|Philip T. Kingston                                                                                                            |\n",
      "|Monica R. Alonzo                                                                                                              |\n",
      "|Michael S. Rawlings                                                                                                           |\n",
      "|Kevin Felder                                                                                                                  |\n",
      "|Adam Medrano                                                                                                                  |\n",
      "|Mark Clayton                                                                                                                  |\n",
      "|Rickey D. Callahan                                                                                                            |\n",
      "|Lee M. Kleinman                                                                                                               |\n",
      "|Erik Wilson                                                                                                                   |\n",
      "|Tennell Atkins                                                                                                                |\n",
      "|Sandy Greyson                                                                                                                 |\n",
      "|  the  final  2018 Assessment Plan and the 2018 Assessment  Roll  (to  be  kept  on  file  with  the  City  Secretary)        |\n",
      "|Jennifer S.  Gates                                                                                                            |\n",
      "|Philip T.  Kingston                                                                                                           |\n",
      "|Jennifer S. Gates                                                                                                             |\n",
      "|Rickey D.  Callahan                                                                                                           |\n",
      "|Lee Kleinman                                                                                                                  |\n",
      "+------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "+-------------------+\n",
      "|VOTER_NAME         |\n",
      "+-------------------+\n",
      "|Mark  Clayton      |\n",
      "|Omar Narvaez       |\n",
      "|Dwaine R. Caraway  |\n",
      "|Casey Thomas       |\n",
      "|Sandy  Greyson     |\n",
      "|Tiffinni A. Young  |\n",
      "|Carolyn King Arnold|\n",
      "|Scott Griggs       |\n",
      "|Scott  Griggs      |\n",
      "|Casey  Thomas      |\n",
      "|B. Adam  McGough   |\n",
      "|Philip T. Kingston |\n",
      "|Monica R. Alonzo   |\n",
      "|Michael S. Rawlings|\n",
      "|Kevin Felder       |\n",
      "|Adam Medrano       |\n",
      "|Mark Clayton       |\n",
      "|Rickey D. Callahan |\n",
      "|Lee M. Kleinman    |\n",
      "|Erik Wilson        |\n",
      "|Tennell Atkins     |\n",
      "|Sandy Greyson      |\n",
      "|Jennifer S.  Gates |\n",
      "|Philip T.  Kingston|\n",
      "|Jennifer S. Gates  |\n",
      "|Rickey D.  Callahan|\n",
      "|Lee Kleinman       |\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the distinct VOTER_NAME entries\n",
    "voter_df.select('VOTER_NAME').distinct().show(40, truncate=False)\n",
    "\n",
    "# Filter voter_df where the VOTER_NAME is 1-20 characters in length\n",
    "voter_df = voter_df.filter('length(VOTER_NAME) > 0 and length(VOTER_NAME) < 20')\n",
    "\n",
    "# Filter out voter_df where the VOTER_NAME contains an underscore\n",
    "voter_df = voter_df.filter(~ F.col('VOTER_NAME').contains('_'))\n",
    "\n",
    "# Show the distinct VOTER_NAME entries again\n",
    "voter_df.select('VOTER_NAME').distinct().show(40, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to return only the Name and State fields for any ID greater than 3000, which code snippet meets these requirements?\n",
    "\n",
    "`users_df.filter('ID > 3000').select(\"Name\", \"State\")`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modifying DataFrame columns\n",
    "Previously, you filtered out any rows that didn't conform to something generally resembling a name. Now based on your earlier work, your manager has asked you to create two new columns - `first_name` and `last_name`. She asks you to split the `VOTER_NAME` column into words on any space character. You'll treat the last word as the `last_name`, and all other words as the `first_name`. You'll be using some new functions in this exercise including `.split()`, `.size()`, and `.getItem()`. The `.getItem(index)` takes an integer value to return the appropriately numbered item in the column. The functions `.split()` and `.size()` are in the `pyspark.sql.functions` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T12:35:25.573650Z",
     "start_time": "2021-02-18T12:35:24.771816Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-------------------+----------+---------+\n",
      "|      DATE|        TITLE|         VOTER_NAME|first_name|last_name|\n",
      "+----------+-------------+-------------------+----------+---------+\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|  Jennifer|    Gates|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|    Philip| Kingston|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|   Michael| Rawlings|\n",
      "|02/08/2017|Councilmember|       Adam Medrano|      Adam|  Medrano|\n",
      "|02/08/2017|Councilmember|       Casey Thomas|     Casey|   Thomas|\n",
      "|02/08/2017|Councilmember|Carolyn King Arnold|   Carolyn|   Arnold|\n",
      "|02/08/2017|Councilmember|       Scott Griggs|     Scott|   Griggs|\n",
      "|02/08/2017|Councilmember|   B. Adam  McGough|        B.|  McGough|\n",
      "|02/08/2017|Councilmember|       Lee Kleinman|       Lee| Kleinman|\n",
      "|02/08/2017|Councilmember|      Sandy Greyson|     Sandy|  Greyson|\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|  Jennifer|    Gates|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|    Philip| Kingston|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|   Michael| Rawlings|\n",
      "|02/08/2017|Councilmember|       Adam Medrano|      Adam|  Medrano|\n",
      "|02/08/2017|Councilmember|       Casey Thomas|     Casey|   Thomas|\n",
      "|02/08/2017|Councilmember|Carolyn King Arnold|   Carolyn|   Arnold|\n",
      "|02/08/2017|Councilmember| Rickey D. Callahan|    Rickey| Callahan|\n",
      "|01/11/2017|Councilmember|  Jennifer S. Gates|  Jennifer|    Gates|\n",
      "|04/25/2018|Councilmember|     Sandy  Greyson|     Sandy|  Greyson|\n",
      "|04/25/2018|Councilmember| Jennifer S.  Gates|  Jennifer|    Gates|\n",
      "+----------+-------------+-------------------+----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add a new column called splits separated on whitespace\n",
    "voter_df = voter_df.withColumn(\"splits\", F.split(voter_df.VOTER_NAME, '\\s+'))\n",
    "\n",
    "# Create a new column called first_name based on the first item in splits\n",
    "voter_df = voter_df.withColumn(\"first_name\", voter_df.splits.getItem(0))\n",
    "\n",
    "# Get the last entry of the splits list and create a column called last_name\n",
    "voter_df = voter_df.withColumn(\"last_name\", voter_df.splits.getItem(F.size('splits') - 1))\n",
    "\n",
    "# Drop the splits column\n",
    "voter_df = voter_df.drop('splits')\n",
    "\n",
    "# Show the voter_df DataFrame\n",
    "voter_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional DataFrame column operations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conditional clauses:\n",
    "* `.when()`\n",
    "* `.otherwise()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.when(<if condition>, <then x>)`\n",
    "\n",
    "`df.select(df.Name, df.Age, F.when(df.Age >= 18, \"Adult\"))`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](when_clause_example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`df.select(df.Name, df.Age,\n",
    "           .when(df.Age >= 18, \"Adult\")\n",
    "           .when(df.Age < 18, \"Minor\"))`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`df.select(df.Name, df.Age,\n",
    "           .when(df.Age >= 18, \"Adult\")\n",
    "           .otherwise(\"Minor\"))`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](when_clause_example_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## when() example\n",
    "The `when()` clause lets you conditionally modify a Data Frame based on its content. You'll want to modify our `voter_df` DataFrame to add a random number to any voting member that is defined as a \"Councilmember\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T12:35:35.622458Z",
     "start_time": "2021-02-18T12:35:34.886576Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-------------------+----------+---------+--------------------+\n",
      "|      DATE|        TITLE|         VOTER_NAME|first_name|last_name|          random_val|\n",
      "+----------+-------------+-------------------+----------+---------+--------------------+\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|  Jennifer|    Gates|   0.674547539110537|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|    Philip| Kingston| 0.37137244547814596|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|   Michael| Rawlings|                null|\n",
      "|02/08/2017|Councilmember|       Adam Medrano|      Adam|  Medrano| 0.06825943955148639|\n",
      "|02/08/2017|Councilmember|       Casey Thomas|     Casey|   Thomas|  0.6522310214396713|\n",
      "|02/08/2017|Councilmember|Carolyn King Arnold|   Carolyn|   Arnold|0.040625990148935864|\n",
      "|02/08/2017|Councilmember|       Scott Griggs|     Scott|   Griggs|  0.5519885719191457|\n",
      "|02/08/2017|Councilmember|   B. Adam  McGough|        B.|  McGough|  0.6052389865961758|\n",
      "|02/08/2017|Councilmember|       Lee Kleinman|       Lee| Kleinman| 0.42352415084240924|\n",
      "|02/08/2017|Councilmember|      Sandy Greyson|     Sandy|  Greyson| 0.17524513193834002|\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|  Jennifer|    Gates|  0.4395840798742825|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|    Philip| Kingston|  0.9652361141587936|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|   Michael| Rawlings|                null|\n",
      "|02/08/2017|Councilmember|       Adam Medrano|      Adam|  Medrano|  0.5530065627133761|\n",
      "|02/08/2017|Councilmember|       Casey Thomas|     Casey|   Thomas|0.004459464355247134|\n",
      "|02/08/2017|Councilmember|Carolyn King Arnold|   Carolyn|   Arnold|0.020179892229990615|\n",
      "|02/08/2017|Councilmember| Rickey D. Callahan|    Rickey| Callahan|  0.7653760282650772|\n",
      "|01/11/2017|Councilmember|  Jennifer S. Gates|  Jennifer|    Gates|  0.9082373875360197|\n",
      "|04/25/2018|Councilmember|     Sandy  Greyson|     Sandy|  Greyson| 0.15818034156757566|\n",
      "|04/25/2018|Councilmember| Jennifer S.  Gates|  Jennifer|    Gates|  0.7017292039668931|\n",
      "+----------+-------------+-------------------+----------+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add a column to voter_df for any voter with the title **Councilmember**\n",
    "voter_df = voter_df.withColumn('random_val',\n",
    "                               F.when(voter_df.TITLE=='Councilmember', F.rand()))\n",
    "\n",
    "# Show some of the DataFrame rows, noting whether the when clause worked\n",
    "voter_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When / Otherwise\n",
    "This requirement is similar to the last, but now you want to add multiple values based on the voter's position. Modify your `voter_df` DataFrame to add a random number to any voting member that is defined as a `Councilmember`. Use 2 for the `Mayor` and 0 for anything other position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Add a column to voter_df named `random_val` with the results of the `F.rand()` method for any voter with the title Councilmember. Set `random_val` to 2 for the Mayor. Set any other title to the value 0.\n",
    "* Show some of the Data Frame rows, noting whether the clauses worked.\n",
    "* Use the `.filter` clause to find 0 in `random_val`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T12:37:52.048739Z",
     "start_time": "2021-02-18T12:37:42.691477Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-------------------+----------+---------+--------------------+\n",
      "|      DATE|        TITLE|         VOTER_NAME|first_name|last_name|          random_val|\n",
      "+----------+-------------+-------------------+----------+---------+--------------------+\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|  Jennifer|    Gates|  0.2800703955427465|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|    Philip| Kingston|  0.6567583884005201|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|   Michael| Rawlings|                 2.0|\n",
      "|02/08/2017|Councilmember|       Adam Medrano|      Adam|  Medrano|0.056485536081120946|\n",
      "|02/08/2017|Councilmember|       Casey Thomas|     Casey|   Thomas| 0.11924405661951543|\n",
      "|02/08/2017|Councilmember|Carolyn King Arnold|   Carolyn|   Arnold|  0.3647192865244855|\n",
      "|02/08/2017|Councilmember|       Scott Griggs|     Scott|   Griggs|  0.9766897561647625|\n",
      "|02/08/2017|Councilmember|   B. Adam  McGough|        B.|  McGough| 0.48555629626510455|\n",
      "|02/08/2017|Councilmember|       Lee Kleinman|       Lee| Kleinman| 0.14937916420446895|\n",
      "|02/08/2017|Councilmember|      Sandy Greyson|     Sandy|  Greyson|0.002728269245758974|\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|  Jennifer|    Gates|  0.6639425856029655|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|    Philip| Kingston|  0.9681161667473703|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|   Michael| Rawlings|                 2.0|\n",
      "|02/08/2017|Councilmember|       Adam Medrano|      Adam|  Medrano|  0.1565686771030962|\n",
      "|02/08/2017|Councilmember|       Casey Thomas|     Casey|   Thomas|  0.6705542083045064|\n",
      "|02/08/2017|Councilmember|Carolyn King Arnold|   Carolyn|   Arnold|   0.720908979456577|\n",
      "|02/08/2017|Councilmember| Rickey D. Callahan|    Rickey| Callahan| 0.14614499206375897|\n",
      "|01/11/2017|Councilmember|  Jennifer S. Gates|  Jennifer|    Gates|  0.9518140310211731|\n",
      "|04/25/2018|Councilmember|     Sandy  Greyson|     Sandy|  Greyson|  0.7301537555763331|\n",
      "|04/25/2018|Councilmember| Jennifer S.  Gates|  Jennifer|    Gates|  0.8860170854049932|\n",
      "+----------+-------------+-------------------+----------+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----------+--------------------+-----------------+----------+---------+----------+\n",
      "|      DATE|               TITLE|       VOTER_NAME|first_name|last_name|random_val|\n",
      "+----------+--------------------+-----------------+----------+---------+----------+\n",
      "|04/25/2018|Deputy Mayor Pro Tem|     Adam Medrano|      Adam|  Medrano|       0.0|\n",
      "|04/25/2018|       Mayor Pro Tem|Dwaine R. Caraway|    Dwaine|  Caraway|       0.0|\n",
      "|06/20/2018|Deputy Mayor Pro Tem|     Adam Medrano|      Adam|  Medrano|       0.0|\n",
      "|06/20/2018|       Mayor Pro Tem|Dwaine R. Caraway|    Dwaine|  Caraway|       0.0|\n",
      "|06/20/2018|Deputy Mayor Pro Tem|     Adam Medrano|      Adam|  Medrano|       0.0|\n",
      "|06/20/2018|       Mayor Pro Tem|Dwaine R. Caraway|    Dwaine|  Caraway|       0.0|\n",
      "|08/15/2018|Deputy Mayor Pro Tem|     Adam Medrano|      Adam|  Medrano|       0.0|\n",
      "|08/15/2018|Deputy Mayor Pro Tem|     Adam Medrano|      Adam|  Medrano|       0.0|\n",
      "|09/18/2018|Deputy Mayor Pro Tem|     Adam Medrano|      Adam|  Medrano|       0.0|\n",
      "|09/18/2018|       Mayor Pro Tem|    Casey  Thomas|     Casey|   Thomas|       0.0|\n",
      "|04/25/2018|Deputy Mayor Pro Tem|     Adam Medrano|      Adam|  Medrano|       0.0|\n",
      "|04/25/2018|       Mayor Pro Tem|Dwaine R. Caraway|    Dwaine|  Caraway|       0.0|\n",
      "|04/11/2018|       Mayor Pro Tem|Dwaine R. Caraway|    Dwaine|  Caraway|       0.0|\n",
      "|04/11/2018|Deputy Mayor Pro Tem|     Adam Medrano|      Adam|  Medrano|       0.0|\n",
      "|04/11/2018|       Mayor Pro Tem|Dwaine R. Caraway|    Dwaine|  Caraway|       0.0|\n",
      "|04/11/2018|Deputy Mayor Pro Tem|     Adam Medrano|      Adam|  Medrano|       0.0|\n",
      "|04/11/2018|       Mayor Pro Tem|Dwaine R. Caraway|    Dwaine|  Caraway|       0.0|\n",
      "|06/13/2018|Deputy Mayor Pro Tem|     Adam Medrano|      Adam|  Medrano|       0.0|\n",
      "|06/13/2018|       Mayor Pro Tem|Dwaine R. Caraway|    Dwaine|  Caraway|       0.0|\n",
      "|04/11/2018|Deputy Mayor Pro Tem|     Adam Medrano|      Adam|  Medrano|       0.0|\n",
      "+----------+--------------------+-----------------+----------+---------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add a column to voter_df for a voter based on their position\n",
    "voter_df = voter_df.withColumn('random_val',\n",
    "                               F.when(voter_df.TITLE == 'Councilmember', F.rand())\n",
    "                               .when(voter_df.TITLE == 'Mayor', 2)\n",
    "                               .otherwise(0))\n",
    "\n",
    "# Show some of the DataFrame rows\n",
    "voter_df.show()\n",
    "\n",
    "# Use the .filter() clause with random_val\n",
    "voter_df.filter('random_val == 0').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User defined functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Define a Python method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T12:38:20.453736Z",
     "start_time": "2021-02-18T12:38:20.451158Z"
    }
   },
   "outputs": [],
   "source": [
    "def reverseString(mystr):\n",
    "    return mystr[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T12:39:57.419899Z",
     "start_time": "2021-02-18T12:39:57.417366Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Wrap the function and store as a variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T12:39:58.637975Z",
     "start_time": "2021-02-18T12:39:58.634193Z"
    }
   },
   "outputs": [],
   "source": [
    "udfReverseString = udf(reverseString, StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Use with Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df = user_df.withColumn('ReverseName',\n",
    "                             udfReverseString(user_df.Name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "defsortingCap():\n",
    "    return random.choice(['G', 'H', 'R', 'S'])\n",
    "\n",
    "udfSortingCap = udf(sortingCap, StringType())\n",
    "user_df = user_df.withColumn('Class', udfSortingCap())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using user defined functions in Spark\n",
    "You've seen some of the power behind Spark's built-in string functions when it comes to manipulating DataFrames. However, once you reach a certain point, it becomes difficult to process the data in a without creating a rat's nest of function calls. Here's one place where you can use User Defined Functions to manipulate our DataFrames.\n",
    "\n",
    "For this exercise, we'll use our `voter_df` DataFrame, but you're going to replace the `first_name` column with the first and middle names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T12:41:16.513993Z",
     "start_time": "2021-02-18T12:41:16.453476Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add a new column called splits separated on whitespace\n",
    "voter_df = voter_df.withColumn(\"splits\", F.split(voter_df.VOTER_NAME, '\\s+'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T12:42:03.959868Z",
     "start_time": "2021-02-18T12:42:02.996743Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-------------------+----------+---------+--------------------+---------------------+\n",
      "|      DATE|        TITLE|         VOTER_NAME|first_name|last_name|          random_val|first_and_middle_name|\n",
      "+----------+-------------+-------------------+----------+---------+--------------------+---------------------+\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|  Jennifer|    Gates|  0.2800703955427465|          Jennifer S.|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|    Philip| Kingston|  0.6567583884005201|            Philip T.|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|   Michael| Rawlings|                 2.0|           Michael S.|\n",
      "|02/08/2017|Councilmember|       Adam Medrano|      Adam|  Medrano|0.056485536081120946|                 Adam|\n",
      "|02/08/2017|Councilmember|       Casey Thomas|     Casey|   Thomas| 0.11924405661951543|                Casey|\n",
      "|02/08/2017|Councilmember|Carolyn King Arnold|   Carolyn|   Arnold|  0.3647192865244855|         Carolyn King|\n",
      "|02/08/2017|Councilmember|       Scott Griggs|     Scott|   Griggs|  0.9766897561647625|                Scott|\n",
      "|02/08/2017|Councilmember|   B. Adam  McGough|        B.|  McGough| 0.48555629626510455|              B. Adam|\n",
      "|02/08/2017|Councilmember|       Lee Kleinman|       Lee| Kleinman| 0.14937916420446895|                  Lee|\n",
      "|02/08/2017|Councilmember|      Sandy Greyson|     Sandy|  Greyson|0.002728269245758974|                Sandy|\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|  Jennifer|    Gates|  0.6639425856029655|          Jennifer S.|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|    Philip| Kingston|  0.9681161667473703|            Philip T.|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|   Michael| Rawlings|                 2.0|           Michael S.|\n",
      "|02/08/2017|Councilmember|       Adam Medrano|      Adam|  Medrano|  0.1565686771030962|                 Adam|\n",
      "|02/08/2017|Councilmember|       Casey Thomas|     Casey|   Thomas|  0.6705542083045064|                Casey|\n",
      "|02/08/2017|Councilmember|Carolyn King Arnold|   Carolyn|   Arnold|   0.720908979456577|         Carolyn King|\n",
      "|02/08/2017|Councilmember| Rickey D. Callahan|    Rickey| Callahan| 0.14614499206375897|            Rickey D.|\n",
      "|01/11/2017|Councilmember|  Jennifer S. Gates|  Jennifer|    Gates|  0.9518140310211731|          Jennifer S.|\n",
      "|04/25/2018|Councilmember|     Sandy  Greyson|     Sandy|  Greyson|  0.7301537555763331|                Sandy|\n",
      "|04/25/2018|Councilmember| Jennifer S.  Gates|  Jennifer|    Gates|  0.8860170854049932|          Jennifer S.|\n",
      "+----------+-------------+-------------------+----------+---------+--------------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def getFirstAndMiddle(names):\n",
    "  # Return a space separated string of names\n",
    "  return ' '.join(names[:-1])\n",
    "\n",
    "# Define the method as a UDF\n",
    "udfFirstAndMiddle = F.udf(getFirstAndMiddle, StringType())\n",
    "\n",
    "# Create a new column using your UDF\n",
    "voter_df = voter_df.withColumn('first_and_middle_name', udfFirstAndMiddle(voter_df.splits))\n",
    "voter_df = voter_df.drop('splits')\n",
    "\n",
    "# Show the DataFrame\n",
    "voter_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partitioning and lazy processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding an ID Field\n",
    "When working with data, you sometimes only want to access certain fields and perform various operations. In this case, find all the unique voter names from the DataFrame and add a unique ID number. Remember that Spark IDs are assigned based on the DataFrame partition - as such the ID values may be much greater than the actual number of rows in the DataFrame.\n",
    "\n",
    "With Spark's lazy processing, the IDs are not actually generated until an action is performed and can be somewhat random depending on the size of the dataset.\n",
    "\n",
    "The spark session and a Spark DataFrame `df` containing the `DallasCouncilVotes.csv.gz` file are available in your workspace. The `pyspark.sql.functions` library is available under the alias `F`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T12:42:39.598326Z",
     "start_time": "2021-02-18T12:42:35.652581Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "There are 27 rows in the voter_df DataFrame.\n",
      "\n",
      "+-------------------+------------+\n",
      "|         VOTER_NAME|      ROW_ID|\n",
      "+-------------------+------------+\n",
      "|       Lee Kleinman|249108103168|\n",
      "|Rickey D.  Callahan|240518168576|\n",
      "|  Jennifer S. Gates|223338299392|\n",
      "|Philip T.  Kingston|214748364800|\n",
      "| Jennifer S.  Gates|197568495616|\n",
      "|      Sandy Greyson|188978561024|\n",
      "|     Tennell Atkins|180388626432|\n",
      "|        Erik Wilson|171798691840|\n",
      "|    Lee M. Kleinman|163208757248|\n",
      "| Rickey D. Callahan|154618822656|\n",
      "+-------------------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select all the unique council voters\n",
    "voter_df = voter_df.select(voter_df[\"VOTER_NAME\"]).distinct()\n",
    "\n",
    "# Count the rows in voter_df\n",
    "print(\"\\nThere are %d rows in the voter_df DataFrame.\\n\" % voter_df.count())\n",
    "\n",
    "# Add a ROW_ID\n",
    "voter_df = voter_df.withColumn('ROW_ID', F.monotonically_increasing_id())\n",
    "\n",
    "# Show the rows with 10 highest IDs in the set\n",
    "voter_df.orderBy(voter_df.ROW_ID.desc()).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More ID tricks\n",
    "Once you define a Spark process, you'll likely want to use it many times. Depending on your needs, you may want to start your IDs at a certain value so there isn't overlap with previous runs of the Spark task. This behavior is similar to how IDs would behave in a relational database. You have been given the task to make sure that the IDs output from a monthly Spark task start at the highest value from the previous month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the highest ROW_ID and save it in previous_max_ID\n",
    "previous_max_ID = voter_df_march.select('ROW_ID').rdd.max()[0]\n",
    "\n",
    "# Add a ROW_ID column to voter_df_april starting at the desired value\n",
    "voter_df_april = voter_df_april.withColumn('ROW_ID', F.monotonically_increasing_id() + previous_max_ID)\n",
    "\n",
    "# Show the ROW_ID from both DataFrames and compare\n",
    "voter_df_march.select('ROW_ID').show()\n",
    "voter_df_april.select('ROW_ID').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caching\n",
    "Caching in Spark refers to storing the results of a DataFrame in memory or on disk of the processing nodes in a cluster. Caching improves the speed for subsequent transformations or actions as the data likely no longer needs to be retrieved from the original data source. Using caching reduces the resource utilization of the cluster - there is less need to access the storage, networking, and CPU of the Spark nodes as the data is likely already present."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disadvantages of caching\n",
    "There are a few disadvantages of caching you should be aware of. Very large data sets may not fit in the memory reserved for cached DataFrames. Depending on the later transformations requested, the cache may not do anything to help performance. If a data set does not stay cached in memory, it may be persisted to disk. Depending on the disk configuration of a Spark cluster, this may not be a large performance improvement. If you're reading from a local network resource and have slow local disk I/O, it may be better to avoid caching the objects. Finally, the lifetime of a cached object is not guaranteed. Spark handles regenerating DataFrames for you automatically, but this can cause delays in processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caching a DataFrame\n",
    "You've been assigned a task that requires running several analysis operations on a DataFrame. You've learned that caching can improve performance when reusing DataFrames and would like to implement it.\n",
    "\n",
    "You'll be working with a new dataset consisting of airline departure information. It may have repetitive data and will need to be de-duplicated.\n",
    "\n",
    "The DataFrame `departures_df` is defined, but no actions have been performed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing caching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call `.cache()` on the DataFrame before Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voter_df = spark.read.csv('voter_data.txt.gz')\n",
    "voter_df.cache().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voter_df = voter_df.withColumn('ID', monotonically_increasing_id())\n",
    "voter_df = voter_df.cache()\n",
    "voter_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check `.is_cached` to determine cache status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(voter_df.is_cached)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call `.unpersist()` when finished with DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voter_df.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caching a DataFrame\n",
    "You've been assigned a task that requires running several analysis operations on a DataFrame. You've learned that caching can improve performance when reusing DataFrames and would like to implement it.\n",
    "\n",
    "You'll be working with a new dataset consisting of airline departure information. It may have repetitive data and will need to be de-duplicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T13:56:08.106157Z",
     "start_time": "2021-02-18T13:56:04.157165Z"
    }
   },
   "outputs": [],
   "source": [
    "# departures_df = spark.read.format('csv').options(Header=True).load('AA_DFW_2017_Departures_Short.csv.gz')\n",
    "departures_df = pd.read_csv(\"AA_DFW_2017_Departures_Short.csv\")\n",
    "departures_df = spark.createDataFrame(departures_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T13:56:19.819313Z",
     "start_time": "2021-02-18T13:56:19.816938Z"
    }
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T13:56:34.712203Z",
     "start_time": "2021-02-18T13:56:20.507302Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting 139358 rows took 13.094988 seconds\n",
      "Counting 139358 rows again took 1.103701 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Add caching to the unique rows in departures_df\n",
    "departures_df = departures_df.distinct().cache()\n",
    "\n",
    "# Count the unique rows in departures_df, noting how long the operation takes\n",
    "print(\"Counting %d rows took %f seconds\" % (departures_df.count(), time.time() - start_time))\n",
    "\n",
    "# Count the rows again, noting the variance in time of a cached DataFrame\n",
    "start_time = time.time()\n",
    "print(\"Counting %d rows again took %f seconds\" % (departures_df.count(), time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing a DataFrame from cache\n",
    "You've finished the analysis tasks with the departures_df DataFrame, but have some other processing to do. You'd like to remove the DataFrame from the cache to prevent any excess memory usage on your cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T13:56:52.485655Z",
     "start_time": "2021-02-18T13:56:52.459064Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is departures_df cached?: True\n",
      "Removing departures_df from cache\n",
      "Is departures_df cached?: False\n"
     ]
    }
   ],
   "source": [
    "# Determine if departures_df is in the cache\n",
    "print(\"Is departures_df cached?: %s\" % departures_df.is_cached)\n",
    "print(\"Removing departures_df from cache\")\n",
    "\n",
    "# Remove departures_df from the cache\n",
    "departures_df.unpersist()\n",
    "\n",
    "# Check the cache status again\n",
    "print(\"Is departures_df cached?: %s\" % departures_df.is_cached)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File import performance\n",
    "You've been given a large set of data to import into a Spark DataFrame. You'd like to test the difference in import speed by splitting up the file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important parameters: \n",
    "* Number of objects (Files, Networklocations, etc) \n",
    "     * More objects better than larger ones \n",
    "     * Can import via wildcard \n",
    "     `airport_df = spark.read.csv('airports*.txt.gz')` \n",
    "\n",
    "* General size of objects \n",
    "    * Spark performs better if objects are of similar size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to split objects\n",
    "* Use OS utilities / scripts (split,cut,awk)\n",
    "\n",
    "`split -l 10000 -d largefile chunk-`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Write out to Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv = spark.read.csv('singlelargefile.csv')\n",
    "df_csv.write.parquet('data.parquet')\n",
    "df = spark.read.parquet('data.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File import performance\n",
    "You've been given a large set of data to import into a Spark DataFrame. You'd like to test the difference in import speed by splitting up the file.\n",
    "\n",
    "You have two types of files available: `departures_full.txt.gz` and `departures_xxx.txt.gz` where xxx is 000 - 013. The same number of rows is split between each file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the full and split files into DataFrames\n",
    "full_df = spark.read.csv('departures_full.txt.gz')\n",
    "split_df = spark.read.csv('departures_*.txt.gz')\n",
    "\n",
    "# Print the count and run time for each DataFrame\n",
    "start_time_a = time.time()\n",
    "print(\"Total rows in full DataFrame:\\t%d\" % full_df.count())\n",
    "print(\"Time to run: %f\" % (time.time() - start_time_a))\n",
    "\n",
    "start_time_b = time.time()\n",
    "print(\"Total rows in split DataFrame:\\t%d\" % split_df.count())\n",
    "print(\"Time to run: %f\" % (time.time() - start_time_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Spark configurations\n",
    "You've recently configured a cluster via a cloud provider. Your only access is via the command shell or your python code. You'd like to verify some Spark settings to validate the configuration of the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T14:34:11.355187Z",
     "start_time": "2021-02-18T14:34:11.347465Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: pyspark-shell\n",
      "Driver TCP port: 31095\n",
      "Number of partitions: 200\n"
     ]
    }
   ],
   "source": [
    "# Name of the Spark application instance\n",
    "app_name = spark.conf.get('spark.app.name')\n",
    "\n",
    "# Driver TCP port\n",
    "driver_tcp_port = spark.conf.get('spark.driver.port')\n",
    "\n",
    "# Number of join partitions\n",
    "num_partitions = spark.conf.get('spark.sql.shuffle.partitions')\n",
    "\n",
    "# Show the results\n",
    "print(\"Name: %s\" % app_name)\n",
    "print(\"Driver TCP port: %s\" % driver_tcp_port)\n",
    "print(\"Number of partitions: %s\" % num_partitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance improvements\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal joins\n",
    "You've been given two DataFrames to combine into a single useful DataFrame. Your first task is to combine the DataFrames normally and view the execution plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the flights_df and aiports_df DataFrames\n",
    "normal_df = flights_df.join(airports_df, \\\n",
    "    flights_df[\"Destination Airport\"] == airports_df[\"IATA\"] )\n",
    "\n",
    "# Show the query plan\n",
    "normal_df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using broadcasting on Spark joins\n",
    "Remember that table joins in Spark are split between the cluster workers. If the data is not local, various shuffle operations are required and can have a negative impact on performance. Instead, we're going to use Spark's broadcast operations to give each node a copy of the specified data.\n",
    "\n",
    "A couple tips:\n",
    "\n",
    "* Broadcast the smaller DataFrame. The larger the DataFrame, the more time required to transfer to the worker nodes.\n",
    "* On small DataFrames, it may be better skip broadcasting and let Spark figure out any optimization on its own.\n",
    "* If you look at the query execution plan, a broadcastHashJoin indicates you've successfully configured broadcasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the broadcast method from pyspark.sql.functions\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "# Join the flights_df and airports_df DataFrames using broadcasting\n",
    "broadcast_df = flights_df.join(broadcast(airports_df), \\\n",
    "    flights_df[\"Destination Airport\"] == airports_df[\"IATA\"] )\n",
    "\n",
    "# Show the query plan and compare against the original\n",
    "broadcast_df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing broadcast vs normal joins\n",
    "You've created two types of joins, normal and broadcasted. Now your manager would like to know what the performance improvement is by using Spark optimizations. If the results are promising, you'll be given more opportunity to tweak the Spark setup as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "# Count the number of rows in the normal DataFrame\n",
    "normal_count = normal_df.count()\n",
    "normal_duration = time.time() - start_time\n",
    "\n",
    "start_time = time.time()\n",
    "# Count the number of rows in the broadcast DataFrame\n",
    "broadcast_count = broadcast_df.count()\n",
    "broadcast_duration = time.time() - start_time\n",
    "\n",
    "# Print the counts and the duration of the tests\n",
    "print(\"Normal count:\\t\\t%d\\tduration: %f\" % (normal_count, normal_duration))\n",
    "print(\"Broadcast count:\\t%d\\tduration: %f\" % (broadcast_count, broadcast_duration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complex processing and data pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. What is a data pipeline?\n",
    "Data pipelines are simply the set of steps needed to move from an input data source, or sources, and convert it to the desired output. A data pipeline can consist of any number of steps or components, and can span many systems. For our purposes, well be setting up a data pipeline within Spark, but realize that a full production data pipeline will likely communicate with many systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick pipeline\n",
    "Before you parse some more complex data, your manager would like to see a simple pipeline example including the basic steps. For this example, you'll want to ingest a data file, filter a few rows, add an ID column to it, then write it out as JSON data.\n",
    "\n",
    "The `spark` context is defined, along with the `pyspark.sql.functions` library being aliased as `F` as is customary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T17:20:26.567853Z",
     "start_time": "2021-02-19T17:20:22.648545Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import the data to a DataFrame\n",
    "# departures_df = spark.read.csv('AA_DFW_2015_Departures_Short.csv', header=True)\n",
    "departures_df = pd.read_csv(\"AA_DFW_2017_Departures_Short.csv\")\n",
    "departures_df = spark.createDataFrame(departures_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T17:20:36.498617Z",
     "start_time": "2021-02-19T17:20:36.432378Z"
    }
   },
   "outputs": [],
   "source": [
    "# Remove any duration of 0\n",
    "departures_df = departures_df.filter(departures_df['Actual elapsed time (Minutes)'] > 0)\n",
    "\n",
    "# Add an ID column\n",
    "departures_df = departures_df.withColumn('id', F.monotonically_increasing_id())\n",
    "\n",
    "# Write the file out to JSON format\n",
    "# departures_df.write.json('output.json', mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T17:20:20.734321Z",
     "start_time": "2021-02-19T17:18:42.964Z"
    }
   },
   "source": [
    "## Data handling techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing commented lines\n",
    "Your boss would like you to perform some complex parsing on a new dataset. The data represents annotation data for the ImageNet dataset, but focusing specifically on dog breeds and identifying them in images. Before any actual analysis can occur, you'll need to clear out several components of invalid / incorrect data. The general schema of the document is unknown so you'd like to import the rows into a single column, allowing for quick analysis.\n",
    "\n",
    "To start, you need to remove all commented rows in the dataset.\n",
    "\n",
    "The `spark` context, and the base CSV file (`annotations.csv.gz`) are available for you to work with. The `col` function is also available for use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the file to a DataFrame and perform a row count\n",
    "annotations_df = spark.read.csv('annotations.csv.gz', sep='|')\n",
    "full_count = annotations_df.count()\n",
    "\n",
    "# Count the number of rows beginning with '#'\n",
    "comment_count = annotations_df.filter(col('_c0').startswith('#')).count()\n",
    "\n",
    "# Import the file to a new DataFrame, without commented rows\n",
    "no_comments_df = spark.read.csv('annotations.csv.gz', sep='|', comment='#')\n",
    "\n",
    "# Count the new DataFrame and verify the difference is as expected\n",
    "no_comments_count = no_comments_df.count()\n",
    "print(\"Full count: %d\\nComment count: %d\\nRemaining count: %d\" % (full_count, comment_count, no_comments_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing invalid rows\n",
    "Now that you've successfully removed the commented rows, you have received some information about the general format of the data. There should be at minimum 5 tab separated columns in the DataFrame. Remember that your original DataFrame only has a single column, so you'll need to split the data on the tab (`\\t`) characters.\n",
    "\n",
    "The DataFrame `annotations_df` is already available, with the commented rows removed. The `spark.sql.functions` library is available under the alias `F`. The initial number of rows available in the DataFrame is stored in the variable `initial_count`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split _c0 on the tab character and store the list in a variable\n",
    "tmp_fields = F.split(annotations_df['_c0'], \"\\t\")\n",
    "\n",
    "# Create the colcount column on the DataFrame\n",
    "annotations_df = annotations_df.withColumn('colcount', F.size(tmp_fields))\n",
    "\n",
    "# Remove any rows containing fewer than 5 fields\n",
    "annotations_df_filtered = annotations_df.filter(~ (annotations_df['colcount'] > 5))\n",
    "\n",
    "# Count the number of rows\n",
    "final_count = annotations_df_filtered.count()\n",
    "print(\"Initial count: %d\\nFinal count: %d\" % (initial_count, final_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting into columns\n",
    "You've cleaned up your data considerably by removing the invalid rows from the DataFrame. Now you want to perform some further transformations by generating specific meaningful columns based on the DataFrame content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the content of _c0 on the tab character (aka, '\\t')\n",
    "split_cols = F.split(annotations_df['_c0'], '\\t')\n",
    "\n",
    "# Add the columns folder, filename, width, and height\n",
    "split_df = annotations_df.withColumn('folder', split_cols.getItem(0))\n",
    "split_df = split_df.withColumn('filename', split_cols.getItem(1))\n",
    "split_df = split_df.withColumn('width', split_cols.getItem(2))\n",
    "split_df = split_df.withColumn('height', split_cols.getItem(3))\n",
    "\n",
    "# Add split_cols as a column\n",
    "split_df = split_df.withColumn('split_cols', split_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further parsing\n",
    "You've molded this dataset into a significantly different format than it was before, but there are still a few things left to do. You need to prep the column data for use in later analysis and remove a few intermediary columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retriever(cols, colcount):\n",
    "  # Return a list of dog data\n",
    "  return cols[4:colcount]\n",
    "\n",
    "# Define the method as a UDF\n",
    "udfRetriever = F.udf(retriever, ArrayType(StringType()))\n",
    "\n",
    "# Create a new column using your UDF\n",
    "split_df = split_df.withColumn('dog_list', udfRetriever(split_df.split_cols, split_df.colcount))\n",
    "\n",
    "# Remove the original column, split_cols, and the colcount\n",
    "split_df = split_df.drop('_c0').drop('colcount').drop('split_cols')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Validate rows via join\n",
    "Another example of filtering data is using joins to remove invalid entries. You'll need to verify the folder names are as expected based on a given DataFrame named `valid_folders_df`. The DataFrame `split_df` is as you last left it with a group of split columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the column in valid_folders_df\n",
    "valid_folders_df = valid_folders_df.withColumnRenamed('_c0', 'folder')\n",
    "\n",
    "# Count the number of rows in split_df\n",
    "split_count = split_df.count()\n",
    "\n",
    "# Join the DataFrames\n",
    "joined_df = split_df.join(valid_folders_df, \"folder\")\n",
    "\n",
    "# Compare the number of rows remaining\n",
    "joined_count = joined_df.count()\n",
    "print(\"Before: %d\\nAfter: %d\" % (split_count, joined_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining invalid rows\n",
    "You've successfully filtered out the rows using a join, but sometimes you'd like to examine the data that is invalid. This data can be stored for later processing or for troubleshooting your data sources.\n",
    "\n",
    "You want to find the difference between two DataFrames and store the invalid rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the row counts for each DataFrame\n",
    "split_count = split_df.count()\n",
    "joined_count = joined_df.count()\n",
    "\n",
    "# Create a DataFrame containing the invalid rows\n",
    "invalid_df = split_df.join(joined_df, 'folder', 'left_anti')\n",
    "\n",
    "# Validate the count of the new DataFrame is as expected\n",
    "invalid_count = invalid_df.count()\n",
    "print(\" split_df:\\t%d\\n joined_df:\\t%d\\n invalid_df: \\t%d\" % (split_count, joined_count, invalid_count))\n",
    "\n",
    "# Determine the number of distinct folder rows removed\n",
    "invalid_folder_count = invalid_df.select('folder').distinct().count()\n",
    "print(\"%d distinct invalid folders found\" % invalid_folder_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dog parsing\n",
    "You've done a considerable amount of cleanup on the initial dataset, but now need to analyze the data a bit deeper. There are several questions that have now come up about the type of dogs seen in an image and some details regarding the images. You realize that to answer these questions, you need to process the data into a specific type. Before you can use it, you'll need to create a schema / type to represent the dog details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the dog details and show 10 untruncated rows\n",
    "print(joined_df.select('dog_list').show(10, truncate=False))\n",
    "\n",
    "# Define a schema type for the details in the dog list\n",
    "DogType = StructType([\n",
    "    StructField(\"breed\", StringType(), False),\n",
    "    StructField(\"start_x\", IntegerType(), False),\n",
    "    StructField(\"start_y\", IntegerType(), False),\n",
    "    StructField(\"end_x\", IntegerType(), False),\n",
    "    StructField(\"end_y\", IntegerType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per image count\n",
    "Your next task in building a data pipeline for this dataset is to create a few analysis oriented columns. You've been asked to calculate the number of dogs found in each image based on your `dog_list` column created earlier. You have also created the `DogType` which will allow better parsing of the data within some of the data columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to return the number and type of dogs as a tuple\n",
    "def dogParse(doglist):\n",
    "    dogs = []\n",
    "    for dog in doglist:\n",
    "        (breed, start_x, start_y, end_x, end_y) = dog.split(',')\n",
    "        dogs.append((breed, int(start_x), int(start_y), int(end_x), int(end_y)))\n",
    "    return dogs\n",
    "\n",
    "# Create a UDF\n",
    "udfDogParse = F.udf(dogParse, ArrayType(DogType))\n",
    "\n",
    "# Use the UDF to list of dogs and drop the old column\n",
    "joined_df = joined_df.withColumn('dogs', udfDogParse('dog_list')).drop('dog_list')\n",
    "\n",
    "# Show the number of dogs in the first 10 rows\n",
    "joined_df.select(F.size('dogs')).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Percentage dog pixels\n",
    "The final task for parsing the dog annotation data is to determine the percentage of pixels in each image that represents a dog (or dogs). You'll need to use the various techniques you've learned in this course to help calculate this information and add it as columns for later analysis.\n",
    "\n",
    "To calculate the percentage of pixels, first calculate the total number of pixels representing each dog then sum them for the image. You can calculate the bounding box with the formula:\n",
    "\n",
    "(Xend - Xstart) * (Yend - Ystart)\n",
    "\n",
    "NOTE: You can ignore the possibility of overlapping bounding boxes in this instance.\n",
    "\n",
    "For the percentage, calculate the total number of \"dog\" pixels divided by the total size of the image, multiplied by 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a UDF to determine the number of pixels per image\n",
    "def dogPixelCount(doglist):\n",
    "    totalpixels = 0\n",
    "    for dog in doglist:\n",
    "        totalpixels += (dog[3] - dog[1]) * (dog[4] - dog[2])\n",
    "    return totalpixels\n",
    "\n",
    "# Define a UDF for the pixel count\n",
    "udfDogPixelCount = F.udf(dogPixelCount, IntegerType())\n",
    "joined_df = joined_df.withColumn('dog_pixels', udfDogPixelCount('dogs'))\n",
    "\n",
    "# Create a column representing the percentage of pixels\n",
    "joined_df = joined_df.withColumn('dog_percent', (joined_df.dog_pixels / (joined_df.width * joined_df.height)) * 100)\n",
    "\n",
    "# Show the first 10 annotations with more than 60% dog\n",
    "joined_df.where('dog_percent > 60').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
